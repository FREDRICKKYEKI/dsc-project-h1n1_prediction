{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "8VWCWHobFVyp",
        "outputId": "f86b94f7-7174-4c54-8148-d0bc48588bd0"
      },
      "id": "8VWCWHobFVyp",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         )\n\u001b[0;32m--> 283\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "kN06QakmyTot"
      },
      "id": "kN06QakmyTot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem description\n",
        "Your goal is to predict how likely individuals are to receive their H1N1 and seasonal flu vaccines. Specifically, you'll be predicting two probabilities: one for `h1n1_vaccine` and one for `seasonal_vaccine`.\n",
        "\n",
        "Each row in the dataset represents one person who responded to the National 2009 H1N1 Flu Survey."
      ],
      "metadata": {
        "id": "ADMyyTtZPECM"
      },
      "id": "ADMyyTtZPECM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this competition, there are two target variables:\n",
        "\n",
        "- `h1n1_vaccine` - Whether respondent received H1N1 flu vaccine.\n",
        "- `seasonal_vaccine` - Whether respondent received seasonal flu vaccine.\n",
        "Both are binary variables: 0 = No; 1 = Yes. Some respondents didn't get either vaccine, others got only one, and some got both. This is formulated as a multilabel (and not multiclass) problem."
      ],
      "metadata": {
        "id": "GONiN60ePf_5"
      },
      "id": "GONiN60ePf_5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The features in this dataset\n",
        "You are provided a dataset with 36 columns. The first column respondent_id is a unique and random identifier. The remaining 35 features are described below.\n",
        "\n",
        "For all binary variables: 0 = No; 1 = Yes.\n",
        "\n",
        "- `h1n1_concern` - Level of concern about the H1N1 flu.\n",
        "0 = Not at all concerned; 1 = Not very concerned; 2 = Somewhat concerned; 3 = Very concerned. -> (__categorical__ __Ordinal__)\n",
        "- `h1n1_knowledge` - Level of knowledge about H1N1 flu.\n",
        "0 = No knowledge; 1 = A little knowledge; 2 = A lot of knowledge.\n",
        "- `behavioral_antiviral_meds` - Has taken antiviral medications. (binary) -> (__categorical__ __Nominal__)\n",
        "-`behavioral_avoidance` - Has avoided close contact with others with flu-like symptoms. (binary) -> (__categorical__ __Nominal__)\n",
        "-`behavioral_face_mask` - Has bought a face mask. (binary) -> (__categorical__ __Nominal__)\n",
        "behavioral_wash_hands - Has frequently washed hands or used hand sanitizer. (binary) -> (__categorical__ __Nominal__)\n",
        "-`behavioral_large_gatherings` - Has reduced time at large gatherings. -> (__categorical__ __Nominal__)\n",
        "-`behavioral_outside_home` - Has reduced contact with people outside of own household. -> (__categorical__ __Nominal__)\n",
        "-`behavioral_touch_face` - Has avoided touching eyes, nose, or mouth. -> (__categorical__ __Nominal__)\n",
        "-`doctor_recc_h1n1` - H1N1 flu vaccine was recommended by doctor. -> (__categorical__ __Nominal__)\n",
        "-`doctor_recc_seasonal` - Seasonal flu vaccine was recommended by doctor. -> (__categorical__ __Nominal__)\n",
        "-`chronic_med_condition` - Has any of the following chronic medical conditions: asthma or an other lung condition, diabetes, a heart condition, a kidney condition, sickle cell anemia or other anemia, a neurological or neuromuscular condition, a liver condition, or a weakened immune system caused by a chronic illness or by medicines taken for a chronic illness. -> (__categorical__ __Nominal__)\n",
        "-`child_under_6_months` - Has regular close contact with a child under the age of six months. -> (__categorical__ __Nominal__)\n",
        "-`health_worker` - Is a healthcare worker. -> (__categorical__ __Nominal__)\n",
        "-`health_insurance` - Has health insurance. -> (__categorical__ __Nominal__)\n",
        "-`opinion_h1n1_vacc_effective` - Respondent's opinion about H1N1 vaccine effectiveness.\n",
        "1 = Not at all effective; 2 = Not very effective; 3 = Don't know; 4 = Somewhat effective; 5 = Very effective. -> (__categorical__ __Ordinal__)\n",
        "-`opinion_h1n1_risk` - Respondent's opinion about risk of getting sick with H1N1 flu without vaccine.\n",
        "1 = Very Low; 2 = Somewhat low; 3 = Don't know; 4 = Somewhat high; 5 = Very high. -> (__categorical__ __Ordinal__)\n",
        "-`opinion_h1n1_sick_from_vacc` - Respondent's worry of getting sick from taking H1N1 vaccine.\n",
        "1 = Not at all worried; 2 = Not very worried; 3 = Don't know; 4 = Somewhat worried; 5 = Very worried. -> (__categorical__ __Ordinal__)\n",
        "-`opinion_seas_vacc_effective` - Respondent's opinion about seasonal flu vaccine effectiveness.\n",
        "1 = Not at all effective; 2 = Not very effective; 3 = Don't know; 4 = Somewhat effective; 5 = Very effective. -> (__categorical__ __Ordinal__)\n",
        "-`opinion_seas_risk` - Respondent's opinion about risk of getting sick with seasonal flu without vaccine.\n",
        "1 = Very Low; 2 = Somewhat low; 3 = Don't know; 4 = Somewhat high; 5 = Very high. -> (__categorical__ __Ordinal__)\n",
        "-`opinion_seas_sick_from_vacc` - Respondent's worry of getting sick from taking seasonal flu vaccine.\n",
        "1 = Not at all worried; 2 = Not very worried; 3 = Don't know; 4 = Somewhat worried; 5 = Very worried. -> (__categorical__ __Ordinal__)\n",
        "-`age_group` - Age group of respondent.\n",
        "\n",
        " ```\n",
        " array(['55 - 64 Years', '35 - 44 Years', '18 - 34 Years', '65+Years', '45 - 54 Years'], dtype=object)\n",
        " ```\n",
        "  -> (__categorical__ __Ordinal__)\n",
        "-`education` - Self-reported education level. -> (__categorical__ __Ordinal__)\n",
        "-`race` - Race of respondent. -> (__categorical__ __Nominal__)\n",
        "-`sex` - Sex of respondent. -> (__categorical__ __Nominal__)\n",
        "-`income_poverty` - Household annual income of respondent with respect to 2008 Census poverty thresholds. -> (__categorical__)\n",
        "-`marital_status` - Marital status of respondent. -> (__categorical__ __Nominal__)\n",
        "-`rent_or_own` - Housing situation of respondent. -> (__categorical__ __Nominal__)\n",
        "-`employment_status` - Employment status of respondent. -> (__categorical__ __Nominal__)\n",
        "-`hhs_geo_region` - Respondent's residence using a 10-region geographic classification defined by the U.S. Dept. of Health and Human Services. Values are represented as short random character strings. -> (__categorical__ __Nominal__)\n",
        "-`census_msa` - Respondent's residence within metropolitan statistical areas (MSA) as defined by the U.S. Census. -> (__categorical__ __Nominal__)\n",
        "-`household_adults` - Number of other adults in household, top-coded to 3. -> (__categorical__ __Ordinal__)\n",
        "-`household_children` - Number of children in household, top-coded to 3. -> (__categorical__ __Ordinal__)\n",
        "-`employment_industry` - Type of industry respondent is employed in.  Values are represented as short random character strings. -> (__categorical__ __Nominal__)\n",
        "-`employment_occupation` - Type of occupation of respondent. Values are represented as short random character strings. -> (__categorical__ __Nominal__)"
      ],
      "metadata": {
        "id": "69sHiIzgRYLn"
      },
      "id": "69sHiIzgRYLn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown above, most predictor variables are categorical, some being _Ordinal_ and others _Nominal_."
      ],
      "metadata": {
        "id": "JpgUMZt2_EgX"
      },
      "id": "JpgUMZt2_EgX"
    },
    {
      "cell_type": "code",
      "source": [
        "#  All necessary imports for data preprocessing\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "1jws4Rrr68b8"
      },
      "id": "1jws4Rrr68b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "path = \"./drive/MyDrive/data/\"\n",
        "train_features_df = pd.read_csv(path + \"training_set_features.csv\")\n",
        "train_labels_df = pd.read_csv(path + \"training_set_labels.csv\")"
      ],
      "metadata": {
        "id": "lYldybOJ5z9_"
      },
      "id": "lYldybOJ5z9_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_df.head()"
      ],
      "metadata": {
        "id": "UCdFsZsH6Kdp"
      },
      "id": "UCdFsZsH6Kdp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_df.head()"
      ],
      "metadata": {
        "id": "-jOO544A6qDJ"
      },
      "id": "-jOO544A6qDJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_df.shape, train_labels_df.shape"
      ],
      "metadata": {
        "id": "f30239ox8Ab0"
      },
      "id": "f30239ox8Ab0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_df.info(verbose=True, show_counts=True)"
      ],
      "metadata": {
        "id": "qghACqqo6xun"
      },
      "id": "qghACqqo6xun",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_df.info(verbose=True, show_counts=True)"
      ],
      "metadata": {
        "id": "ldPRLjGU8jFU"
      },
      "id": "ldPRLjGU8jFU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check unique features\n",
        "print(\"- age_group: \", train_features_df[\"age_group\"].unique())\n",
        "print()\n",
        "print(\"- education: \", train_features_df[\"education\"].unique())\n",
        "print()\n",
        "print(\"- race: \", train_features_df[\"race\"].unique())\n",
        "print()\n",
        "print(\"- sex: \", train_features_df[\"sex\"].unique())\n",
        "print()\n",
        "print(\"- income_poverty: \", train_features_df[\"income_poverty\"].unique())\n",
        "print()\n",
        "print(\"- marital_status: \", train_features_df[\"marital_status\"].unique())\n",
        "print()\n",
        "print(\"- rent_or_own: \", train_features_df[\"rent_or_own\"].unique())\n",
        "print()\n",
        "print(\"- employment_status: \", train_features_df[\"employment_status\"].unique())\n",
        "print()\n",
        "print(\"- hhs_geo_region: \", train_features_df[\"hhs_geo_region\"].unique())\n",
        "print()\n",
        "print(\"- household_adults: \", train_features_df[\"household_adults\"].unique())\n",
        "print()\n",
        "print(\"- household_children: \", train_features_df[\"household_children\"].unique())\n",
        "print()\n",
        "print(\"- census_msa: \", train_features_df[\"census_msa\"].unique())\n",
        "print()\n",
        "print(\"- employment_industry: \", train_features_df[\"employment_industry\"].unique())\n",
        "print()\n",
        "print(\"- employment_occupation: \", train_features_df[\"employment_occupation\"].unique())\n",
        "print()"
      ],
      "metadata": {
        "id": "iMBiNIy79uIE"
      },
      "id": "iMBiNIy79uIE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine train and test for preprocessing\n",
        "train_data = pd.merge(train_features_df, train_labels_df, on=\"respondent_id\")\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "tBAcHNuy_sSX"
      },
      "id": "tBAcHNuy_sSX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"Dataframe shape: \", train_data.shape)\n",
        "print(\"\\nMissing Values in Training Features:\")\n",
        "print(train_data.isnull().sum())"
      ],
      "metadata": {
        "id": "e9LLFjDB_b-f"
      },
      "id": "e9LLFjDB_b-f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing (data cleaning)\n",
        "\n"
      ],
      "metadata": {
        "id": "0QjAH3w_LB75"
      },
      "id": "0QjAH3w_LB75"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handle missing values\n",
        "- Step 1. Remove the features with an excessively high number of missing values (typically more than half the length of the data set)."
      ],
      "metadata": {
        "id": "-WzI7TEFsCo4"
      },
      "id": "-WzI7TEFsCo4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with excessive missing values\n",
        "missing_threshold = 0.5\n",
        "train_data = train_data.loc[:, train_data.isnull().mean() < missing_threshold]\n",
        "\n",
        "print(\"New dataframe shape: \", train_data.shape)\n",
        "print(\"\\nMissing Values in Training Features:\")\n",
        "print(train_data.isnull().sum())"
      ],
      "metadata": {
        "id": "cutWhscLBDfz"
      },
      "id": "cutWhscLBDfz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Step 2.  Imputation with \"Unknown\" or Mode**\n",
        "  - **Imputation with \"Unknown\" Category**\n",
        "\n",
        "    For categorical features where the missing value doesn't have an inherent order or ranking (nominal features), it's common to replace missing values with a placeholder category like **\"Unknown.\"** This approach prevents the loss of information and keeps the categorical nature intact.\n",
        "    \n",
        "  - >**Why ?** Since nominal categories don't have a natural order, introducing a new category for missing data ensures that no assumptions are made about the missing values.\n",
        "\n",
        "  - **Imputation with Mode**\n",
        "\n",
        "    For ordinal features (categorical features with a natural order or ranking), it's usually better to impute missing values with the **mode** (the most frequent category). This maintains the integrity of the ordinal nature while filling in missing values with the most likely category.\n",
        "  - >**Why ?** Using the mode preserves the order of the categories and can be more informative than introducing a separate \"Unknown\" category."
      ],
      "metadata": {
        "id": "m7f9oOJvQL2I"
      },
      "id": "m7f9oOJvQL2I"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Separate features and labels\n",
        "train_features_df = train_data.drop(columns=[\"respondent_id\", \"h1n1_vaccine\", \"seasonal_vaccine\"])\n",
        "train_labels_df = train_data[[\"h1n1_vaccine\", \"seasonal_vaccine\"]]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_features_df, train_labels_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify nominal and ordinal features\n",
        "nominal_features = [\n",
        "    'behavioral_antiviral_meds', 'behavioral_avoidance', 'behavioral_face_mask',\n",
        "    'behavioral_wash_hands', 'behavioral_large_gatherings', 'behavioral_outside_home',\n",
        "    'behavioral_touch_face', 'doctor_recc_h1n1', 'doctor_recc_seasonal',\n",
        "    'chronic_med_condition', 'child_under_6_months', 'health_worker',\n",
        "    'health_insurance', 'race', 'sex', 'marital_status', 'rent_or_own',\n",
        "    'employment_status', 'hhs_geo_region', 'census_msa', 'employment_industry'\n",
        "]\n",
        "\n",
        "ordinal_features = [\n",
        "    'h1n1_concern', 'h1n1_knowledge', 'opinion_h1n1_vacc_effective', 'opinion_h1n1_risk',\n",
        "    'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective', 'opinion_seas_risk',\n",
        "    'opinion_seas_sick_from_vacc', 'age_group', 'education', 'income_poverty',\n",
        "    'household_adults', 'household_children'\n",
        "]\n",
        "\n",
        "# Check for all columns\n",
        "assert set(nominal_features + ordinal_features) == set(X_train.columns)\n",
        "\n",
        "# Impute nominal features with 'Unknown'\n",
        "nominal_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
        "X_train[nominal_features] = nominal_imputer.fit_transform(X_train[nominal_features])\n",
        "X_test[nominal_features] = nominal_imputer.transform(X_test[nominal_features])\n",
        "\n",
        "# Impute ordinal features with mode\n",
        "ordinal_imputer = SimpleImputer(strategy='most_frequent')\n",
        "X_train[ordinal_features] = ordinal_imputer.fit_transform(X_train[ordinal_features])\n",
        "X_test[ordinal_features] = ordinal_imputer.transform(X_test[ordinal_features])\n",
        "\n",
        "# After imputations, check the results for any missing values\n",
        "print(\"New training set shape: \", X_train.shape)\n",
        "print(\"New testing set shape: \", X_test.shape)\n",
        "print(\"\\nMissing Values in Training Features:\")\n",
        "print(X_train.isnull().sum())\n",
        "\n",
        "print(\"\\nMissing Values in Testing Features:\")\n",
        "print(X_test.isnull().sum())\n",
        "\n",
        "# merge X_train with y_train and X_test with y_test for further processing\n",
        "train_data_imputed = pd.concat([X_train, y_train], axis=1)\n",
        "test_data_imputed = pd.concat([X_test, y_test], axis=1)"
      ],
      "metadata": {
        "id": "bTyyPy5JRaC4"
      },
      "id": "bTyyPy5JRaC4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert all float data types to ints (they ought to be categorical && discrete)\n",
        "def convert_to_int(x):\n",
        "  if x == \"unknown\":\n",
        "    return x\n",
        "  try:\n",
        "    return int(x)\n",
        "  except ValueError:\n",
        "    return x\n",
        "\n",
        "train_data_imputed = train_data_imputed.applymap(convert_to_int)\n",
        "test_data_imputed = test_data_imputed.applymap(convert_to_int)\n",
        "\n",
        "# inspect once more\n",
        "for col in train_data_imputed.columns:\n",
        "  if col == \"respondent_id\":\n",
        "    continue\n",
        "  print(f\"- {col}: \", train_data_imputed[col].unique())"
      ],
      "metadata": {
        "id": "4jyd72yWCi-F"
      },
      "id": "4jyd72yWCi-F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handle Duplicates"
      ],
      "metadata": {
        "id": "tx7DAuCwr5_q"
      },
      "id": "tx7DAuCwr5_q"
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Duplicates:\n",
        "duplicate_rows = train_data_imputed[train_data_imputed.duplicated()]\n",
        "num_duplicate_rows = len(duplicate_rows)\n",
        "print(f\"Number of duplicate rows (train): {num_duplicate_rows}\")\n",
        "\n",
        "duplicate_rows = test_data_imputed[test_data_imputed.duplicated()]\n",
        "num_duplicate_rows = len(duplicate_rows)\n",
        "print(f\"Number of duplicate rows (test): {num_duplicate_rows}\")"
      ],
      "metadata": {
        "id": "LD1ONfwcouZC"
      },
      "id": "LD1ONfwcouZC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop duplicates\n",
        "train_data_imputed = train_data_imputed.drop_duplicates()\n",
        "test_data_imputed = test_data_imputed.drop_duplicates()\n",
        "\n",
        "# confirm\n",
        "duplicate_rows = train_data_imputed[train_data_imputed.duplicated()]\n",
        "num_duplicate_rows = len(duplicate_rows)\n",
        "print(f\"Number of duplicate rows (train): {num_duplicate_rows}\")"
      ],
      "metadata": {
        "id": "THUGbxlEoQYA"
      },
      "id": "THUGbxlEoQYA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explorative Data Analysis\n",
        "First we perform some Exploratory Data Analysis as an initial step to check what our data\n",
        "contains. This will provide us with valuable insights into what the data contains and its basic\n",
        "characteristics. Here's what we do during the EDA:"
      ],
      "metadata": {
        "id": "SULZRFoKIAok"
      },
      "id": "SULZRFoKIAok"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Univariate Analysis\n"
      ],
      "metadata": {
        "id": "6w-qKaPVIXQ5"
      },
      "id": "6w-qKaPVIXQ5"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Drop 'respondent_id', 'h1n1_vaccine', 'seasonal_vaccine' from the features\n",
        "train_features_df = train_data_imputed.drop(columns=[\"h1n1_vaccine\", \"seasonal_vaccine\"])\n",
        "train_labels_df = train_data_imputed[[\"h1n1_vaccine\", \"seasonal_vaccine\"]]\n",
        "\n",
        "train_feat_cols = list(train_features_df.columns)\n",
        "\n",
        "# Function to create a pie chart for binary variables\n",
        "def plot_pie_chart(data: pd.DataFrame, column: str, ax: plt.Axes, title: str) -> None:\n",
        "  \"\"\"\n",
        "  plot pie chart for binary variables\n",
        "  :param data: dataframe\n",
        "  :param column: column name\n",
        "  :param ax: axis\n",
        "  :param title: title of the plot\n",
        "  \"\"\"\n",
        "  labels = ['No', 'Yes']\n",
        "\n",
        "  if column == \"sex\":\n",
        "      labels = ['Female', 'Male']\n",
        "\n",
        "  counts = data[column].value_counts()\n",
        "  sizes = counts.values\n",
        "  colors = ['#ff9999', '#66b3ff']\n",
        "\n",
        "  ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "  ax.axis('equal')  # Equal aspect ratio ensures that pie chart is circular.\n",
        "  ax.set_title(title)\n",
        "\n",
        "# Set up a grid of subplots with 3 columns per row\n",
        "n_cols = 3\n",
        "n_rows = (len(train_feat_cols) + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(18, n_rows * 4))\n",
        "\n",
        "# Flatten the axes array for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Create plots for each categorical variable\n",
        "for i, var in enumerate(train_feat_cols):\n",
        "    if train_features_df[var].nunique() == 2:  # Check if the variable is binary\n",
        "        plot_pie_chart(train_features_df, var, axes[i], f'Proportion of {var}')\n",
        "    else:\n",
        "        sns.countplot(data=train_features_df, x=var, ax=axes[i])\n",
        "        axes[i].set_title(f'Countplot of {var}')\n",
        "        axes[i].set_xlabel(var)\n",
        "        axes[i].set_ylabel('Count')\n",
        "        axes[i].tick_params(axis='x', rotation=45)  # Rotate x-tick labels for better readability\n",
        "\n",
        "# Hide any unused subplots\n",
        "\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8uDDdT7lJ_3_"
      },
      "id": "8uDDdT7lJ_3_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_df.describe()"
      ],
      "metadata": {
        "id": "wsNvkWivT4Yt"
      },
      "id": "wsNvkWivT4Yt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the mappings for ordinal features\n",
        "ordinal_mappings = {\n",
        "    'h1n1_concern': {\n",
        "        'Not at all concerned': 0,\n",
        "        'Not very concerned': 1,\n",
        "        'Somewhat concerned': 2,\n",
        "        'Very concerned': 3\n",
        "    },\n",
        "    'h1n1_knowledge': {\n",
        "        'No knowledge': 0,\n",
        "        'Some knowledge': 1,\n",
        "        'Knowledgeable': 2\n",
        "    },\n",
        "    'opinion_h1n1_vacc_effective': {\n",
        "        'Not effective at all': 0,\n",
        "        'Not very effective': 1,\n",
        "        'Somewhat effective': 2,\n",
        "        'Very effective': 3\n",
        "    },\n",
        "    'opinion_h1n1_risk': {\n",
        "        'No risk': 0,\n",
        "        'Low risk': 1,\n",
        "        'Moderate risk': 2,\n",
        "        'High risk': 3\n",
        "    },\n",
        "    'opinion_h1n1_sick_from_vacc': {\n",
        "        'Not at all likely': 0,\n",
        "        'Not very likely': 1,\n",
        "        'Somewhat likely': 2,\n",
        "        'Very likely': 3\n",
        "    },\n",
        "    'opinion_seas_vacc_effective': {\n",
        "        'Not effective at all': 0,\n",
        "        'Not very effective': 1,\n",
        "        'Somewhat effective': 2,\n",
        "        'Very effective': 3\n",
        "    },\n",
        "    'opinion_seas_risk': {\n",
        "        'No risk': 0,\n",
        "        'Low risk': 1,\n",
        "        'Moderate risk': 2,\n",
        "        'High risk': 3\n",
        "    },\n",
        "    'opinion_seas_sick_from_vacc': {\n",
        "        'Not at all likely': 0,\n",
        "        'Not very likely': 1,\n",
        "        'Somewhat likely': 2,\n",
        "        'Very likely': 3\n",
        "    },\n",
        "    'age_group': {\n",
        "        '18 - 34 Years': 0,\n",
        "        '35 - 44 Years': 1,\n",
        "        '45 - 54 Years': 2,\n",
        "        '55 - 64 Years': 3,\n",
        "        '65+ Years': 4\n",
        "    },\n",
        "    'education': {\n",
        "        '< 12 Years': 0,\n",
        "        '12 Years': 1,\n",
        "        'Some College': 2,\n",
        "        'College Graduate': 3,\n",
        "        'Some Post-Graduate': 4\n",
        "    },\n",
        "    'income_poverty': {\n",
        "        'Below Poverty': 0,\n",
        "        '<= $75,000, Above Poverty': 1,\n",
        "        '> $75,000': 2\n",
        "    },\n",
        "    'household_adults': {\n",
        "        '1 Adult': 0,\n",
        "        '2 Adults': 1,\n",
        "        '3+ Adults': 2\n",
        "    },\n",
        "    'household_children': {\n",
        "        '0 Children': 0,\n",
        "        '1 Child': 1,\n",
        "        '2 Children': 2,\n",
        "        '3+ Children': 3\n",
        "    }\n",
        "}\n",
        "\n",
        "# Apply the mappings using replace function\n",
        "for feature, mapping in ordinal_mappings.items():\n",
        "    train_data_imputed[feature] = train_data_imputed[feature].replace(mapping)\n",
        "    test_data_imputed[feature] = test_data_imputed[feature].replace(mapping)\n",
        "\n",
        "# Verify the changes\n",
        "print(train_data_imputed[list(ordinal_mappings.keys())].head())\n",
        "print(test_data_imputed[list(ordinal_mappings.keys())].head())\n"
      ],
      "metadata": {
        "id": "CbDSHdyIdu1B"
      },
      "id": "CbDSHdyIdu1B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. label encode categorical variables\n",
        "\n",
        "# One-hot encode all categorical features in the train and test datasets\n",
        "train_data_encoded = pd.get_dummies(train_data_imputed, drop_first=True)\n",
        "test_data_encoded = pd.get_dummies(test_data_imputed, drop_first=True)\n",
        "\n",
        "# Ensure that both the train and test datasets have the same columns after encoding\n",
        "test_data_encoded = test_data_encoded.reindex(columns=train_data_encoded.columns, fill_value=0)\n",
        "\n",
        "print(\"Shape of encoded train data:\", train_data_encoded.shape)\n",
        "print(\"Shape of encoded test data:\", test_data_encoded.shape)"
      ],
      "metadata": {
        "id": "P3oIZ6zpx5FR"
      },
      "id": "P3oIZ6zpx5FR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bivariate analysis"
      ],
      "metadata": {
        "id": "bvjJ84mJpwFP"
      },
      "id": "bvjJ84mJpwFP"
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_encoded.columns"
      ],
      "metadata": {
        "id": "i6zRMWM596p0"
      },
      "id": "i6zRMWM596p0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix\n",
        "correlation_matrix = train_data_imputed[ordinal_features].corr()\n",
        "correlation_matrix\n"
      ],
      "metadata": {
        "id": "LGvce1OvRONz"
      },
      "id": "LGvce1OvRONz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the matplotlib figure\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8}, linewidths=0.5)\n",
        "\n",
        "# Title and labels\n",
        "plt.title('Heatmap of Correlations Between Ordinal Features')\n",
        "# plt.xticks(rotation=45)\n",
        "# plt.yticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1eY-HkBJR8GL"
      },
      "id": "1eY-HkBJR8GL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Insights from the correlation matrix:__\n",
        "\n",
        "1. **Strongest Positive Correlations**:\n",
        "   - **`opinion_h1n1_risk` vs `opinion_seas_risk` (0.566)**: Individuals who perceive a higher risk from the H1N1 vaccine are likely to also perceive a higher risk from the seasonal flu vaccine.\n",
        "   - **`opinion_h1n1_sick_from_vacc` vs `opinion_seas_sick_from_vacc` (0.486)**: A belief that one might get sick from the H1N1 vaccine is associated with a similar belief regarding the seasonal flu vaccine.\n",
        "   - **`opinion_h1n1_vacc_effective` vs `opinion_seas_vacc_effective` (0.470)**: Individuals who believe the H1N1 vaccine is effective are also more likely to believe in the effectiveness of the seasonal flu vaccine.\n",
        "\n",
        "2. **Moderate Positive Correlations**:\n",
        "   - **`opinion_h1n1_risk` vs `h1n1_concern` (0.380)**: Those who are more concerned about H1N1 are also likely to believe there is a higher risk associated with it.\n",
        "   - **`opinion_h1n1_sick_from_vacc` vs `opinion_h1n1_risk` (0.336)**: Those who believe they could get sick from the H1N1 vaccine are also more likely to perceive the risk of H1N1 as higher.\n",
        "\n",
        "   - **`opinion_h1n1_risk` vs `opinion_h1n1_vacc_effective` (0.260)**: Those who perceive a higher risk from H1N1 are more likely to believe that the vaccine is effective.\n",
        "\n",
        "\n",
        "3. **Negative Correlations**:\n",
        "   - **`age_group` vs `household_children` (-0.438)**: Older individuals tend to have fewer children in their households.\n",
        "   - **`education` vs `age_group` (-0.081)**: Higher education levels are weakly negatively correlated with age, indicating that older individuals in the dataset might have slightly lower education levels.\n",
        "   - **`education` vs `opinion_h1n1_sick_from_vacc` (-0.088)**: Higher education levels are weakly associated with a lower belief that the H1N1 vaccine could make someone sick.\n",
        "\n",
        "4. **Weak or Negligible Correlations**:\n",
        "   - Many features, such as `household_adults` and `income_poverty` or `opinion_seas_vacc_effective`, have very weak correlations with other features, indicating that these features may not strongly influence each other.\n",
        "\n",
        "5. **Educational Insights**:\n",
        "   - **`education` vs `income_poverty` (0.372)**: Higher education is moderately correlated with lower poverty levels, which is expected.\n",
        "\n",
        "6. **Perception of Vaccines**:\n",
        "   - The correlations between perceptions of the H1N1 vaccine and the seasonal flu vaccine (e.g., effectiveness, risk, and sickness from the vaccine) suggest that individuals’ views on one vaccine are often mirrored in their views on the other. This could be due to general attitudes towards vaccination.\n",
        "\n",
        "7. **Household Dynamics**:\n",
        "   - **`household_children` vs `age_group` (-0.438)**: As people get older, they tend to have fewer children in the household, which makes sense given typical family life cycles.\n",
        "\n",
        "**Summary:**\n",
        "- **Perception Patterns**: There are clear patterns in how people perceive the risks and benefits of vaccines, with those who are concerned about one aspect (e.g., risk) being concerned about related aspects (e.g., getting sick from the vaccine).\n",
        "- **Educational and Age Factors**: Higher education seems to buffer against concerns about vaccine safety, while age is inversely related to having children in the household.\n",
        "- **Poverty and Education**: As expected, higher education correlates with lower levels of poverty, showing socioeconomic impacts on health-related perceptions.\n",
        "\n",
        "These insights can help in targeting public health messages, understanding vaccine hesitancy, and tailoring interventions to specific demographic groups."
      ],
      "metadata": {
        "id": "Bo8dEn9plIif"
      },
      "id": "Bo8dEn9plIif"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictive Modelling"
      ],
      "metadata": {
        "id": "9k-i0yPbsyRl"
      },
      "id": "9k-i0yPbsyRl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proposed models\n",
        "This modelling requires a multilabel classifier. Some models we can use are listed below:\n",
        "\n",
        "1. **Random Forest with MultiOutputClassifier**\n",
        "\n",
        "2. **XGBoost with MultiOutputClassifier**\n",
        "\n",
        "3. **Neural Networks**\n"
      ],
      "metadata": {
        "id": "tJx9dSD53NbF"
      },
      "id": "tJx9dSD53NbF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Multioutput classifier"
      ],
      "metadata": {
        "id": "6R-RlDoTrOQ2"
      },
      "id": "6R-RlDoTrOQ2"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "SEED = 555"
      ],
      "metadata": {
        "id": "AsLGcbUS0C1X"
      },
      "id": "AsLGcbUS0C1X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_encoded = train_data_encoded.drop(columns=[\"h1n1_vaccine\", \"seasonal_vaccine\"])\n",
        "y_train = train_data_encoded[[\"h1n1_vaccine\", \"seasonal_vaccine\"]]\n",
        "\n",
        "X_test_encoded = test_data_encoded.drop(columns=[\"h1n1_vaccine\", \"seasonal_vaccine\"])\n",
        "y_test = test_data_encoded[[\"h1n1_vaccine\", \"seasonal_vaccine\"]]"
      ],
      "metadata": {
        "id": "ZxZPFYkJ4J3q"
      },
      "id": "ZxZPFYkJ4J3q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=SEED)\n",
        "\n",
        "# Wrap it in a MultiOutputClassifier\n",
        "multi_target_rf = MultiOutputClassifier(rf, n_jobs=-1)\n",
        "\n",
        "# Train the model\n",
        "multi_target_rf.fit(X_train_encoded, y_train)"
      ],
      "metadata": {
        "id": "ovHV136P3-aq"
      },
      "id": "ovHV136P3-aq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test data\n",
        "y_pred = multi_target_rf.predict(X_test_encoded)"
      ],
      "metadata": {
        "id": "fh-wofWU4sG6"
      },
      "id": "fh-wofWU4sG6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the columns by name\n",
        "y_pred_h1n1 = y_pred[:, 0]\n",
        "y_pred_seasonal = y_pred[:, 1]\n",
        "\n",
        "# Accuracy for each target\n",
        "accuracy_h1n1 = accuracy_score(y_test['h1n1_vaccine'], y_pred_h1n1)\n",
        "accuracy_seasonal = accuracy_score(y_test['seasonal_vaccine'], y_pred_seasonal)\n",
        "\n",
        "print(\"Untuned RF model\\n\"+\"=\"*20)\n",
        "print(f'Accuracy for H1N1 vaccine prediction: {accuracy_h1n1:.2f}')\n",
        "print(f'Accuracy for Seasonal vaccine prediction: {accuracy_seasonal:.2f}')\n",
        "\n",
        "# Classification reports\n",
        "print(\"Classification Report for H1N1 Vaccine Prediction:\")\n",
        "print(classification_report(y_test['h1n1_vaccine'], y_pred_h1n1))\n",
        "\n",
        "print(\"\\nClassification Report for Seasonal Vaccine Prediction:\")\n",
        "print(classification_report(y_test['seasonal_vaccine'], y_pred_seasonal))\n",
        "\n",
        "# Confusion matrices\n",
        "print(\"Confusion Matrix for H1N1 Vaccine Prediction:\")\n",
        "print(confusion_matrix(y_test['h1n1_vaccine'], y_pred_h1n1))\n",
        "\n",
        "print(\"\\nConfusion Matrix for Seasonal Vaccine Prediction:\")\n",
        "print(confusion_matrix(y_test['seasonal_vaccine'], y_pred_seasonal))\n"
      ],
      "metadata": {
        "id": "e4fSqLDH4wxs"
      },
      "id": "e4fSqLDH4wxs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tuned Random Forest classifier"
      ],
      "metadata": {
        "id": "HKnzcqr2syoo"
      },
      "id": "HKnzcqr2syoo"
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# # Define the parameter grid\n",
        "# param_grid = {\n",
        "#     'n_estimators': [100, 200],\n",
        "#     'max_depth': [None, 10, 20],\n",
        "#     'min_samples_split': [5, 10],\n",
        "#     'min_samples_leaf': [1, 2],\n",
        "# }\n",
        "# # Initialize the Random Forest model\n",
        "# rf = RandomForestClassifier(random_state=SEED)\n",
        "\n",
        "# # Set up the GridSearchCV with the RandomForest and the parameter grid\n",
        "# grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
        "#                            cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
        "\n",
        "# # Fit the grid search\n",
        "# grid_search.fit(X_train_encoded, y_train)\n",
        "\n",
        "# # Best parameters from the grid search\n",
        "# best_params = grid_search.best_params_\n",
        "\n",
        "# # Best score from the grid search\n",
        "# best_score = grid_search.best_score_\n",
        "\n",
        "# print(\"Best Parameters:\", best_params)\n",
        "# print(\"Best Cross-Validation Score:\", best_score)\n"
      ],
      "metadata": {
        "id": "vgGc0t7Qs3lY"
      },
      "id": "vgGc0t7Qs3lY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extended Gradient Boosting (XGBoost) Multioutput classifier"
      ],
      "metadata": {
        "id": "zxjCEjg6rY_R"
      },
      "id": "zxjCEjg6rY_R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Below is the output of the above grid search.\n",
        "The GridSearch is commented out because it takes a very long time to find the best parameters for the Random Forest Classifier.\n",
        "\n",
        ">Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
        "Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
        "Best Cross-Validation Score: 0.6815207916444253"
      ],
      "metadata": {
        "id": "jwRixGDZ--YS"
      },
      "id": "jwRixGDZ--YS"
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_rf = RandomForestClassifier(**best_params, random_state=SEED)\n",
        "best_rf.fit(X_train_encoded, y_train)"
      ],
      "metadata": {
        "id": "UZevvw3h-q6q"
      },
      "id": "UZevvw3h-q6q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the tuned model on the test set\n",
        "y_test_array = y_test.values  # Convert to a NumPy array\n",
        "accuracy_h1n1 = accuracy_score(y_test_array[:, 0], y_pred[:, 0])\n",
        "accuracy_seasonal = accuracy_score(y_test_array[:, 1], y_pred[:, 1])\n",
        "\n",
        "print(\"Tuned Random Forest Multiclassifier\\n\"+ \"=\"*40)\n",
        "print(f\"Accuracy for H1N1 vaccine prediction: {accuracy_h1n1:.2f}\")\n",
        "print(f\"Accuracy for Seasonal vaccine prediction: {accuracy_seasonal:.2f}\")"
      ],
      "metadata": {
        "id": "uIXZCQJNAuWT"
      },
      "id": "uIXZCQJNAuWT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Wrap with MultiOutputClassifier\n",
        "multi_target_xgb = MultiOutputClassifier(xgb_model, n_jobs=-1)\n",
        "\n",
        "# Train the model\n",
        "multi_target_xgb.fit(X_train_encoded, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_xgb = multi_target_xgb.predict(X_test_encoded)\n",
        "\n",
        "# Evaluate the model\n",
        "# Accuracy for each target\n",
        "accuracy_h1n1 = accuracy_score(y_test['h1n1_vaccine'], y_pred_xgb[:, 0])\n",
        "accuracy_seasonal = accuracy_score(y_test['seasonal_vaccine'], y_pred_xgb[:, 1])\n",
        "\n",
        "print(\"Untuned XGboosted model\\n\" + \"=\"*20)\n",
        "print(f\"Accuracy for H1N1 vaccine prediction: {accuracy_h1n1:.2f}\")\n",
        "print(f\"Accuracy for Seasonal vaccine prediction: {accuracy_seasonal:.2f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report for H1N1 Vaccine Prediction:\")\n",
        "print(classification_report(y_test['h1n1_vaccine'], y_pred_xgb[:, 0]))\n",
        "\n",
        "print(\"Classification Report for Seasonal Vaccine Prediction:\")\n",
        "print(classification_report(y_test['seasonal_vaccine'], y_pred_xgb[:, 1]))\n",
        "\n",
        "# Confusion matrices\n",
        "print(\"Confusion Matrix for H1N1 Vaccine Prediction:\")\n",
        "print(confusion_matrix(y_test['h1n1_vaccine'], y_pred_xgb[:, 0]))\n",
        "\n",
        "print(\"Confusion Matrix for Seasonal Vaccine Prediction:\")\n",
        "print(confusion_matrix(y_test['seasonal_vaccine'], y_pred_xgb[:, 1]))\n"
      ],
      "metadata": {
        "id": "8RNRtPwWrlbp"
      },
      "id": "8RNRtPwWrlbp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tuned Xgboosted model"
      ],
      "metadata": {
        "id": "DJt11ZPjt2DL"
      },
      "id": "DJt11ZPjt2DL"
    },
    {
      "cell_type": "code",
      "source": [
        "# from xgboost import XGBClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# # Define the parameter grid\n",
        "# param_grid = {\n",
        "#     'estimator__n_estimators': [100, 200, 300],\n",
        "#     'estimator__max_depth': [3, 5, 7],\n",
        "#     'estimator__learning_rate': [0.01, 0.09],\n",
        "#     'estimator__subsample': [0.8, 1.0],\n",
        "#     'estimator__colsample_bytree': [0.8, 1.0],\n",
        "#     # 'estimator__gamma': [0, 0.1, 0.2],\n",
        "#     # 'estimator__reg_alpha': [0, 0.1, 1],\n",
        "#     # 'estimator__reg_lambda': [1, 1.5, 2]\n",
        "# }\n",
        "\n",
        "# # Initialize the XGBoost model\n",
        "# xgb = XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# # Wrap XGBoost with MultiOutputClassifier\n",
        "# multi_xgb = MultiOutputClassifier(xgb)\n",
        "\n",
        "# # Set up the GridSearchCV\n",
        "# grid_search = GridSearchCV(estimator=multi_xgb, param_grid=param_grid,\n",
        "#                            cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
        "\n",
        "# # Fit the grid search\n",
        "# grid_search.fit(X_train_encoded, y_train)\n",
        "\n",
        "# # Best parameters from the grid search\n",
        "# best_params = grid_search.best_params_\n",
        "\n",
        "# # Best score from the grid search\n",
        "# best_score = grid_search.best_score_\n",
        "\n",
        "# print(\"Best Parameters:\", best_params)\n",
        "# print(\"Best Cross-Validation Score:\", best_score)"
      ],
      "metadata": {
        "id": "SS7pzl05t_th"
      },
      "id": "SS7pzl05t_th",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output of Xgboost Grid Search CV\n",
        "\n",
        ">Fitting 5 folds for each of 72 candidates, totalling 360 fits.\\\n",
        "Best Parameters: {'estimator__colsample_bytree': 0.8, 'estimator__learning_rate': 0.09, 'estimator__max_depth': 3, 'estimator__n_estimators': 300, 'estimator__subsample': 0.8}\\\n",
        "Best Cross-Validation Score: 0.6922398480661167"
      ],
      "metadata": {
        "id": "VOYlyGllKeWg"
      },
      "id": "VOYlyGllKeWg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with the best parameters\n",
        "best_params = {'estimator__colsample_bytree': 0.8, 'estimator__learning_rate': 0.09,\n",
        "               'estimator__max_depth': 3, 'estimator__n_estimators': 300,\n",
        "               'estimator__subsample': 0.8}\n",
        "\n",
        "best_xgb = MultiOutputClassifier(XGBClassifier(**best_params,\n",
        "                                               objective='binary:logistic',\n",
        "                                               use_label_encoder=False,\n",
        "                                               eval_metric='logloss'))\n",
        "\n",
        "best_xgb.fit(X_train_encoded, y_train)\n",
        "\n",
        "\n",
        "# Evaluate the tuned model on the test set\n",
        "y_pred = best_xgb.predict(X_test_encoded)\n",
        "\n",
        "# Calculate accuracy for H1N1 and Seasonal predictions\n",
        "accuracy_h1n1 = accuracy_score(y_test.iloc[:, 0], y_pred[:, 0])\n",
        "accuracy_seasonal = accuracy_score(y_test.iloc[:, 1], y_pred[:, 1])\n",
        "\n",
        "print(\"Tuned XGboosted model\\n\" + \"=\"*20)\n",
        "print(f\"Accuracy for H1N1 vaccine prediction: {accuracy_h1n1:.2f}\")\n",
        "print(f\"Accuracy for Seasonal vaccine prediction: {accuracy_seasonal:.2f}\")"
      ],
      "metadata": {
        "id": "rZgloh5BBQqO"
      },
      "id": "rZgloh5BBQqO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Learning Model\n"
      ],
      "metadata": {
        "id": "NEBKd9NuMBIz"
      },
      "id": "NEBKd9NuMBIz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Design\n",
        "\n",
        "Here's how we will design the Base Neural Network model:\n",
        "\n",
        "- **Input Layer:** The number of nodes in the input layer should match the number of features (84 in your case).\n",
        "- **Hidden Layers:** We start with a simple architecture, like 2-3 hidden layers with 64-128 neurons each, and experiment with different architectures later.\n",
        "- **Output Layer:** Since we have two target variables (H1N1 and Seasonal vaccine), we use two output neurons with a sigmoid activation function."
      ],
      "metadata": {
        "id": "wPsor4SiOiuq"
      },
      "id": "wPsor4SiOiuq"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout"
      ],
      "metadata": {
        "id": "1o7XYiNrNqja"
      },
      "id": "1o7XYiNrNqja",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_training_results(results):\n",
        "    \"\"\"Function to visualize model performance\n",
        "    \"\"\"\n",
        "    history = results.history\n",
        "    plt.figure()\n",
        "    plt.plot(history['loss'], label='train_loss')\n",
        "    plt.plot(history['val_loss'], label='val_loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(history['accuracy'], label='train_accuracy')\n",
        "    plt.plot(history['val_accuracy'], label='val_accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "aaYRUWTypY1v"
      },
      "id": "aaYRUWTypY1v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer and first hidden layer\n",
        "model.add(Dense(128, input_dim=X_train_encoded.shape[1], activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "# model.add(Dropout(0.2))  # Dropout for regularization\n",
        "\n",
        "# Second hidden layer\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Third hidden layer (optional)\n",
        "model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "jrNM6wj3N9EB"
      },
      "id": "jrNM6wj3N9EB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model compilation\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "gWRDqH5zN-j1"
      },
      "id": "gWRDqH5zN-j1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_encoded_int = X_train_encoded.astype('int')\n",
        "y_train_int = y_train.astype('int')\n",
        "\n",
        "results_nn_model_1 = model.fit(X_train_encoded_int, y_train_int,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=100,\n",
        "                    batch_size=5,\n",
        "                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])"
      ],
      "metadata": {
        "id": "oFZrdgyiOujE",
        "collapsed": true
      },
      "id": "oFZrdgyiOujE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_training_results(results_nn_model_1)"
      ],
      "metadata": {
        "id": "RAQ5JGx9paqM"
      },
      "id": "RAQ5JGx9paqM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from lightgbm import LGBMClassifier\n",
        "\n",
        "# # Convert feature names to a DataFrame if needed\n",
        "# X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=[f\"feature_{i}\" for i in range(X_train_encoded.shape[1])])\n",
        "# X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=[f\"feature_{i}\" for i in range(X_test_encoded.shape[1])])\n",
        "\n",
        "# # Rename columns to remove special characters (if any)\n",
        "# X_train_encoded_df.columns = X_train_encoded_df.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
        "# X_test_encoded_df.columns = X_test_encoded_df.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
        "\n",
        "# # Convert back to numpy array if needed\n",
        "# X_train_encoded = X_train_encoded_df.values\n",
        "# X_test_encoded = X_test_encoded_df.values\n",
        "\n",
        "# lgbm_model = MultiOutputClassifier(LGBMClassifier(\n",
        "#     random_state=SEED,\n",
        "#     min_data_in_bin=3,\n",
        "#     min_data_in_leaf=10,\n",
        "#     num_leaves=31,\n",
        "#     learning_rate=0.05,\n",
        "#     feature_fraction=0.9,\n",
        "#     n_estimators=100))\n",
        "# lgbm_model.fit(X_train_encoded, y_train)\n",
        "\n",
        "# y_pred = lgbm_model.predict(X_test_encoded)"
      ],
      "metadata": {
        "id": "1oYNp_7TB5Fp"
      },
      "id": "1oYNp_7TB5Fp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support Vector Machine"
      ],
      "metadata": {
        "id": "VDYMqWY3ITFh"
      },
      "id": "VDYMqWY3ITFh"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "svm_model = OneVsRestClassifier(SVC(kernel='linear', probability=True))\n",
        "svm_model.fit(X_train_encoded, y_train)\n",
        "\n",
        "y_pred = svm_model.predict(X_test_encoded)"
      ],
      "metadata": {
        "id": "FL1t5U964MQu"
      },
      "id": "FL1t5U964MQu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy for H1N1 and Seasonal predictions\n",
        "accuracy_h1n1 = accuracy_score(y_test.iloc[:, 0], y_pred[:, 0])\n",
        "accuracy_seasonal = accuracy_score(y_test.iloc[:, 1], y_pred[:, 1])\n",
        "\n",
        "print(\"Untuned SVM model\\n\" + \"=\"*20)\n",
        "print(f\"Accuracy for H1N1 vaccine prediction: {accuracy_h1n1:.2f}\")\n",
        "print(f\"Accuracy for Seasonal vaccine prediction: {accuracy_seasonal:.2f}\")"
      ],
      "metadata": {
        "id": "sF-Pfp504PSi"
      },
      "id": "sF-Pfp504PSi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train_encoded, y_train)\n",
        "\n",
        "# Print the best parameters and the best score\n",
        "print(\"Best Parameters: \", grid_search.best_params_)\n",
        "print(\"Best Accuracy: \", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "sVs498mwIxpq",
        "outputId": "cd792b38-44e5-4255-f8e3-812c1a7211d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "id": "sVs498mwIxpq",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'GridSearchCV' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e99b9abaa286>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize GridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fit GridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}